{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. K-Nearest Neighbors Regression (KNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-Nearest Neighbors Regression este un algoritm de invatare nesupervizata, conceput pentru a fi utilizat în probleme care implică segmentarea pieței, detectarea fraudei, motoare de recomandare, gruparea paginilor web prin similitudine.\n",
    "\n",
    "În KNN, K reprezinta numărul de vecini apropiați. Numărul de vecini este factorul decisiv de bază. K este, în general, un număr impar dacă numărul de clase este 2. Când K = 1 algoritmul este cunoscut ca nearest neighbor. Acesta este cel mai simplu caz. Sa presupunem că P1 este un punct pentru care trebuie sa ii prezicem eticheta. Mai întâi, găsim cel mai apropiat punct de P1 și atribuim eticheta acestuia lui P1.\n",
    "\n",
    "![report](http://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1531424125/Knn_k1_z96jba.png)\n",
    "\n",
    "Cand K > 1 algoritmul cauta K cei mai apropiati vecini de punctul P1 si eticheta lui este bazata pe votul majoritatii vecinilor. Pentru a gasi cele mai apropiate puncte similare de P1, trebuie calculate distantele dintre puncte folosind metrici precum distanta Euclidiana, distanta Hamming, distanta Manhattan si distanta Minkowski. KNN are urmatoarele etape de baza:\n",
    "\n",
    "1. Calculculeaza distanta\n",
    "2. Gaseste cel mai apropiat vecin\n",
    "3. Voteaza pentru eticheta\n",
    "\n",
    "![report](http://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1531424125/KNN_final1_ibdm8a.png)\n",
    "\n",
    "Nu este nevoie de învățare sau instruire a modelului și a tuturor punctelor de date folosite la momentul predicției. Algoritmul  așteaptă până în ultimul minut pentru a clasifica orice punct. Acesta stochează doar setul de date de antrenament și așteaptă până când clasificarea trebuie să se desfășoare. Doar atunci când vede setul de testare, efectuează generalizarea pentru a clasifica setul de testare bazat pe similitudinea cu setul de antrenament stocat. Spre deosebire de alte metodele de învățare, algoritmul face mai puțini pasi în faza de formare și mai multă muncă în faza de testare pentru a face o clasificare.\n",
    "\n",
    "K-nearest neighbors regression folosește ponderi uniforme, adică fiecare punct al unei clase contribuie uniform la identificarea unui punct de interogare. În anumite circumstanțe, poate fi avantajos să se analizeze puntele astfel încât punctele din apropiere să contribuie mai mult la regresie decât punctele îndepărtate. Acest lucru poate fi realizat prin intermediul cuvântului cheie ponderi. Valoarea implicită, weights = \"uniform\", atribuie ponderi egale tuturor punctelor. weights = 'distance' atribuie ponderi proporționale cu inversul distanței de la punctul de interogare. Alternativ, poate fi furnizată o funcție definită de utilizator a distanței, care va fi utilizată pentru a calcula ponderile.\n",
    "\n",
    "Cercetările au arătat că nici un număr optim de vecini nu se potrivește cu toate tipurile de seturi de date. Fiecare set de date are propriile cerințe. În cazul unui număr mic de vecini, zgomotul va avea o influență mai mare asupra rezultatului, și un număr mare de vecini îl face costisitor din punct de vedere computațional. Cercetările au arătat, de asemenea, că o cantitate mică de vecini este cea mai flexibilă. În general, se alege K un număr impar dacă numărul de clase este par.\n",
    "\n",
    "\n",
    "Resurse:\n",
    "1. https://www.datacamp.com/community/tutorials/k-nearest-neighbor-classification-scikit-learn\n",
    "2. https://analyticstraining.com/popular-regression-algorithms-ml/\n",
    "3. https://scikit-learn.org/stable/modules/neighbors.html#regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Support Vector Regression (SVR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support Vector Regression este utilizat în machine learning pentru a rezolva problemele legate de segmentarea imaginii, piața bursieră, clasificarea textului și științele biologice.\n",
    "\n",
    "Support Vector Machines este o clasa foarte specifica de algoritmi, caracterizata prin folosirea kernelurilor, absenta minimelor locale, dispersie a solutiei si controlul capacitatii obtinute prin actiunea asupra marginilor sau pe numarul de support vectors etc.\n",
    "\n",
    "SVR folosește aceleași principii ca SVM pentru clasificare, cu doar câteva diferențe minore. În primul rând, deoarece output-ul este un număr real, devine foarte dificilă prezicerea informațiilor care au posibilități infinite. În cazul regresiei, o marjă de toleranță (epsilon) este stabilită în apropierea SVM care ar fi solicitat deja din problemă. Cu toate acestea, ideea principală este întotdeauna aceeași: pentru a minimiza eroarea se individualizeaza hiperplanul care maximizează marja, având în vedere că o parte din eroare este tolerată. \n",
    "\n",
    "*Kernel-ul este funcția utilizată pentru a mapa date dimensionale inferioare într-o valoare dimensională superioară.\n",
    "\n",
    "*Hyper Plane in SVR este definit ca linia care ne va ajuta să anticipăm valoarea continuă sau valoarea țintă.\n",
    "\n",
    "*Support Vectors sunt punctele care se află cel mai aproape de limită. Distanța punctelor este minimă.\n",
    "\n",
    "*Boundary line: în SVM există alte două linii decât Hyper Plane care creează o marjă. Support Vectors pot fi pe Boundary line sau în afara ei. Această linie de separare separă cele două clase. În SVR, conceptul este același.\n",
    "\n",
    "Exista 2 tipuri de SVR:\n",
    "1. Linear\n",
    "2. Non-Linear\n",
    "\n",
    "![raport](https://www.saedsayad.com/images/SVR_5.png)\n",
    "\n",
    "A se vedea modul în care toate punctele se află între boundaryes lines. Obiectivul nostru este ca atunci când ne mișcăm cu SVR sa luam în considerare punctele care se află între boundaryes lines. Linia de potrivire cea mai bună este linia hyperplane care are un număr maxim de puncte.\n",
    "\n",
    "![](https://cdn-images-1.medium.com/freeze/1*rs0EfF8RPVpgA-EfgAq85g.jpeg)\n",
    "\n",
    "Modelul produs de Support Vector Regression depinde numai de un subset de date de antrenament, deoarece funcția de cost pentru construirea modelului ignoră toate datele de antrenament apropiate de predicția modelului.\n",
    "\n",
    "Resurse:\n",
    "1. https://medium.com/coinmonks/support-vector-regression-or-svr-8eb3acf6d0ff\n",
    "2. https://analyticstraining.com/popular-regression-algorithms-ml/\n",
    "3. https://www.saedsayad.com/support_vector_machine_reg.htm\n",
    "4. https://scikit-learn.org/stable/modules/svm.html#svm-regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Random Forest Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest Regressor este utilizat într-o mulțime de domenii diferite, cum ar fi sectorul bancar, bursa, medicina și comerțul electronic. În sectorul bancar se utilizează, de exemplu, pentru a detecta clienții care vor folosi serviciile băncii mai frecvent decât alții și își vor rambursa datoriile în timp. În acest domeniu, este folosit și pentru detectarea clienților de fraudă care doresc să înșele banca. În finanțe, este folosit pentru a determina comportamentul stocului în viitor. În domeniul sănătății este folosit pentru a identifica combinația corectă a componentelor din medicină și pentru a analiza istoricul medical al pacientului pentru a identifica bolile. Și, în sfârșit, in comerțul electronic este utilizat pentru a determina dacă un client va dori de fapt produsul sau nu.\n",
    "\n",
    "Random forest este o tehnică de ansamblu capabilă să efectueze atât sarcini de regresie, cât și sarcini de clasificare, folosind mai mulți copaci de decizie și o tehnică numită Bootstrap Aggregation, cunoscută în mod obișnuit ca \"bagging\". Bagging-ul, în metoda Random Forest, implică instruirea fiecărui arbore al deciziilor pe un eșantion de date diferit, unde prelevarea de probe se face cu înlocuirea.\n",
    "\n",
    "Ideea de bază din spatele acestui algoritm este combinarea mai multor arbori de decizie în determinarea output-ului final, în loc să se bazeze pe arborii decizionali individuali.\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/1200/1*i0o8mjFfCn-uD79-F1Cqkw.png)\n",
    "\n",
    "Random forest adaugă o randomizare suplimentară modelului, în timp ce crește copacii. În loc să căutăm cea mai importantă caracteristică în timp ce divizăm un nod, acesta caută cea mai bună caracteristică dintr-un subset aleatoriu de caracteristici. Aceasta are ca rezultat o diversitate largă care, în general, are ca rezultat un model mai bun.\n",
    "\n",
    "Prin urmare, în random forest, doar un subset aleator al caracteristicilor este luat în considerare prin algoritmul de împărțire a unui nod. Se pot face arbori mai aleatorii, folosind în plus praguri aleatorii pentru fiecare caracteristică, în loc să căutați cele mai bune praguri posibile (cum face un decision tree).\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/1600/1*9kACduxnce_JdTrftM_bsA.gif)\n",
    "\n",
    "Resuse:\n",
    "1. https://medium.com/datadriveninvestor/random-forest-regression-9871bc9a25eb\n",
    "2. https://towardsdatascience.com/the-random-forest-algorithm-d457d499ffcd\n",
    "3. https://medium.com/@williamkoehrsen/random-forest-simple-explanation-377895a60d2d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Decision Tree Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree este un model intuitiv în care se traversează cate o data ramurile copacului și selectează următoarea ramură pentru a merge în jos pe baza unei decizii la un nod. Inducția copacilor este sarcina de a lua un set de instanțe de formare ca intrări, de a decide care atribute sunt cel mai bine să se împartă, de a împărți setul de date și de a repeta pe seturile de date divizate rezultate până când toate instanțele de formare sunt clasificate. \n",
    "\n",
    "În timpul construirii copacului, scopul este de a se împărți pe atributele care creează cele mai pure noduri copil posibile, ceea ce ar reduce la minimum numărul de împărțiri care ar trebui făcute pentru a clasifica toate instanțele din setul nostru de date. Puritatea este măsurată prin conceptul de câștig de informații, care se referă la cât de mult ar trebui să fie cunoscuta o instanță anterior nevăzută pentru a fi clasificată corespunzător. \n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/800/1*KOg1s4s4mg44q_6Oi5D-mQ.png)\n",
    "\n",
    "În practică, acest lucru se măsoară prin compararea entropiei sau a cantității de informații necesare pentru a clasifica o singură instanță a unei partiții de date curente la cantitatea de informații pentru a clasifica o singură instanță în cazul în care divizia de seturi de date curente urma să fie mai mult împărțită pe un anumit atribut.\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/600/1*XMId5sJqPtm8-RIwVVz2tg.png)\n",
    "\n",
    "O problemă des intalnita este faptul ca un set mare de caracteristici, rezultă un număr mare de împărțire, care la rândul său conduce la un copac uriaș. Asemenea copaci sunt complexi și pot duce la suprasolicitare. Deci, trebuie să știm când să ne oprim. O modalitate de a face acest lucru este de a stabili un număr minim de intrări de instruire pe fiecare frunză. De exemplu, putem folosi cel puțin 10 pasageri pentru a ajunge la o decizie (a murit sau a supraviețuit) și pentru a ignora orice frunză care ia mai puțin de 10 pasageri. O altă modalitate este de a stabili adâncimea maximă a modelului. Adâncimea maximă se referă la lungimea celei mai lungi căi de la o rădăcină la o frunză.\n",
    "\n",
    "Resurse:\n",
    "1. https://towardsdatascience.com/selecting-the-best-machine-learning-algorithm-for-your-regression-problem-20c330bad4ef\n",
    "2. https://towardsdatascience.com/decision-trees-in-machine-learning-641b9c4e8052\n",
    "3. https://scikit-learn.org/stable/modules/tree.html#tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Gaussian Process Regression (GPR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gaussian Process este un algoritm puternic atât pentru regresie, cât și pentru clasificare. Cel mai mare avantaj practic al acestuia este că poate oferi o estimare fiabilă a propriei incertitudini. \n",
    "\n",
    "Gaussian Process Regressor implementează Gaussian Process (GP) în scopuri de regresie. Pentru aceasta, trebuie precizat înainte GP. Media anterioară se presupune a fi constantă și zero (pentru normalize_y = False) sau media datelor de antrenament (pentru normalize_y = True). Co-covarianța precedentului este specificată prin trecerea unui obiect kernel. Hiperparametrele kernel-ului sunt optimizate în timpul montajului GPR prin maximizarea probabilității log-marginale (LML) bazată pe optimizatorul trecut. Deoarece LML poate avea mai multe optimizări locale, optimizatorul poate fi pornit în mod repetat prin specificarea n_restarts_optimizer. Prima rulare se efectuează întotdeauna pornind de la valorile inițiale ale hiperparametrului kernel-ului; etapele următoare sunt efectuate din valorile hiperparametrului care au fost alese aleatoriu din intervalul de valori permise. Dacă hiperparametrele inițiale ar trebui să fie ținute fixe, niciuna nu poate fi trecută ca optimizator.\n",
    "\n",
    "Nivelul de zgomot poate fi specificat prin trecerea acestuia prin parametrul alfa, fie global ca scalar, fie per datapoint. Rețineți că un nivel moderat al zgomotului poate fi, de asemenea, util pentru rezolvarea problemelor numerice în timpul montajului, deoarece este efectiv implementat ca regularizare Tikhonov, adică prin adăugarea acestuia la diagonala matricei kernel-ului. O alternativă la specificarea nivelului de zgomot în mod explicit este includerea unei componente WhiteKernel în kernel, care poate estima nivelul global de zgomot din date.\n",
    "\n",
    "Acest exemplu ilustrează faptul că GPR cu un kernel sum care include un WhiteKernel poate estima nivelul de zgomot al datelor. O ilustrare a peisajului logargin-probabilității (LML) arată că există două maxime locale ale LML.\n",
    "\n",
    "![](https://scikit-learn.org/stable/_images/sphx_glr_plot_gpr_noisy_0001.png)\n",
    "\n",
    "Primul corespunde unui model cu un nivel ridicat al zgomotului și o scală de lungime mare, care explică toate variațiile datelor prin zgomot.\n",
    "\n",
    "![](https://scikit-learn.org/stable/_images/sphx_glr_plot_gpr_noisy_0011.png)\n",
    "\n",
    "Cel de-al doilea are un nivel de zgomot mai mic și o scală de lungime mai scurtă, ceea ce explică cea mai mare parte a variației prin relația funcțională fără zgomot. Cel de-al doilea model are o probabilitate mai mare; totuși, în funcție de valoarea inițială pentru hiperparametrii, optimizarea pe bază de gradient ar putea converge, de asemenea, la soluția cu zgomot ridicat. Este deci important să repetați optimizarea de mai multe ori pentru diferite inițializări.\n",
    "\n",
    "Resurse:\n",
    "1. https://scikit-learn.org/stable/modules/gaussian_process.html\n",
    "2. https://towardsdatascience.com/an-intuitive-guide-to-gaussian-processes-ec2f0b45c71d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "notify_time": "5",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
